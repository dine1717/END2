{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S5 Assignmnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dine1717/END2/blob/Session5/S5_Assignmnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r2GKxd57bg3"
      },
      "source": [
        "# Load required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "from statistics import mean,median,mode\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data\n",
        "\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mywYNfa-MOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c472447-8105-4922-edcc-db6fb4bbd8c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z41Qnogms9X"
      },
      "source": [
        "**Unzip Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKY5hMuJFKWN",
        "outputId": "108563cf-bd0e-4914-8a7d-06f3eaf92681"
      },
      "source": [
        "!unzip /content/drive/MyDrive/stanfordSentimentTreebank.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/stanfordSentimentTreebank.zip\n",
            "   creating: stanfordSentimentTreebank/\n",
            "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/stanfordSentimentTreebank/\n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
            "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
            "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
            "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
            "  inflating: stanfordSentimentTreebank/README.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
            "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
            "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
            "  inflating: stanfordSentimentTreebank/STree.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJppbPSV_RYe",
        "outputId": "fa359436-102e-4b11-db91-40fd86ed49c4"
      },
      "source": [
        "dataset_sentences = pd.read_csv('stanfordSentimentTreebank/datasetSentences.txt', sep='\\t')\n",
        "print(f'{dataset_sentences.shape}   {dataset_sentences.head()}')\n",
        "sentiment_labels = pd.read_csv('stanfordSentimentTreebank/sentiment_labels.txt', sep='|')                \n",
        "sentiment_labels = sentiment_labels.rename(columns={'phrase ids': 'phrase_id', \n",
        "                                                    'sentiment values': 'sentiment_value'})\n",
        "print(f'{sentiment_labels.shape}   {sentiment_labels.head()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11855, 2)      sentence_index                                           sentence\n",
            "0               1  The Rock is destined to be the 21st Century 's...\n",
            "1               2  The gorgeously elaborate continuation of `` Th...\n",
            "2               3                     Effective but too-tepid biopic\n",
            "3               4  If you sometimes like to go to the movies to h...\n",
            "4               5  Emerges as something rare , an issue movie tha...\n",
            "(239232, 2)      phrase_id  sentiment_value\n",
            "0          0          0.50000\n",
            "1          1          0.50000\n",
            "2          2          0.44444\n",
            "3          3          0.50000\n",
            "4          4          0.42708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PZil3G2Fx_Zz",
        "outputId": "d86f0b99-1f9e-46aa-c560-e1ff9dc27776"
      },
      "source": [
        "dataset_sentences.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpWjmxaeyiiQ",
        "outputId": "d48c9656-8bb8-4150-c11f-adbff30fc7d0"
      },
      "source": [
        "dataset_sentences.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "yWrpbaOqyHF0",
        "outputId": "995f594e-7319-449c-ab53-abba8ef5a29d"
      },
      "source": [
        "sentiment_labels.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase_id</th>\n",
              "      <th>sentiment_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   phrase_id  sentiment_value\n",
              "0          0          0.50000\n",
              "1          1          0.50000\n",
              "2          2          0.44444\n",
              "3          3          0.50000\n",
              "4          4          0.42708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ZxK5G0ylZD",
        "outputId": "8cd6171e-d476-477e-af32-2d0d77e7a9ce"
      },
      "source": [
        "sentiment_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(239232, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht65ByEm_awk"
      },
      "source": [
        "**Mappping Sentiments from Phrases to Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY74z8Iiy0S9"
      },
      "source": [
        "sentence_sentiment = dataset_sentences\n",
        "phrase_sentiment = dict(zip(list(sentiment_labels.phrase_id), \n",
        "                            list(sentiment_labels.sentiment_value)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skbXbZAg_bVV"
      },
      "source": [
        "# average sentiments per sentence (otherwise long sentences get high sentiments)\n",
        "sentiments = [mean(phrase_sentiment[int(phrase_id)] for phrase_id in phrase_ids) \n",
        "              for phrase_ids in stree]\n",
        "# define neutral sentiment as values within [0.5, 0.55) \n",
        "sentiments = [2 if sentiment >= 0.5 and sentiment < 0.55 else sentiment \n",
        "              for sentiment in sentiments]\n",
        "# all negative sentiment values to 0, all positive sentiment values to 1\n",
        "sentiments = [int(round(sentiment)) \n",
        "              for sentiment in sentiments]\n",
        "# concatenate sentence sentiment values with corresponding sentence texts\n",
        "sentence_sentiment['sentiment_value'] = sentiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWDDESnZvcEh",
        "outputId": "95716b03-03f2-410a-ed47-402fefd6e6b9"
      },
      "source": [
        "min(sentence_sentiment['sentiment_value']), max((sentence_sentiment['sentiment_value']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.420832, 0.6521161904761905)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXosmoI1utJo"
      },
      "source": [
        "sentence_sentiment['label'] = sentence_sentiment.apply(lambda row: score_to_label(row['sentiment_value']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty6oNDUt_h6Q",
        "outputId": "da4b1fb7-a45b-4d6c-89e6-c82e2ca7a6e4"
      },
      "source": [
        "print((100 * sentence_sentiment.sentiment_value.value_counts(normalize=True)).map('{:0.2f}%'.format))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.581417    494\n",
            "0.492492    490\n",
            "0.522222    484\n",
            "0.532828    467\n",
            "0.598764    465\n",
            "0.543906    458\n",
            "0.489018    451\n",
            "0.484331    450\n",
            "0.616666    442\n",
            "0.462738    430\n",
            "0.619565    428\n",
            "0.491358    424\n",
            "0.465721    402\n",
            "0.652116    401\n",
            "0.652046    401\n",
            "0.643791    377\n",
            "0.439909    359\n",
            "0.622221    354\n",
            "0.449347    331\n",
            "0.455975    330\n",
            "0.482322    319\n",
            "0.519229    310\n",
            "0.449496    310\n",
            "0.462964    248\n",
            "0.454919    215\n",
            "0.450095    214\n",
            "0.444444    209\n",
            "0.451941    205\n",
            "0.450855    186\n",
            "0.452383    182\n",
            "0.443616    155\n",
            "0.440419    126\n",
            "0.420832    114\n",
            "0.453443    101\n",
            "0.470001     84\n",
            "0.463471     81\n",
            "0.470058     51\n",
            "0.468107     49\n",
            "0.500000     48\n",
            "0.475035     44\n",
            "0.451635     33\n",
            "0.455824     32\n",
            "0.464241     28\n",
            "0.476592     22\n",
            "0.479244     14\n",
            "0.489474     11\n",
            "0.493128      9\n",
            "0.484469      5\n",
            "0.489899      4\n",
            "0.491750      4\n",
            "0.502428      3\n",
            "0.479229      1\n",
            "Name: sentiment_value, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1GiNYLsK_pe6",
        "outputId": "815b6199-98cc-4143-b285-46ed69106bb3"
      },
      "source": [
        "# rename and select columns\n",
        "\n",
        "sentence_sentiment = sentence_sentiment.rename(columns={'sentence': 'tweets', 'sentiment_value': 'labels'})\n",
        "sentence_sentiment = sentence_sentiment[['tweets', 'labels']]\n",
        "sentence_sentiment.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7842</th>\n",
              "      <td>Nothing more substantial than a fitfully cleve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2955</th>\n",
              "      <td>Last Orders nurtures the multi-layers of its c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7911</th>\n",
              "      <td>The film seems a dead weight .</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3884</th>\n",
              "      <td>The performances are all solid ; it merely lac...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2922</th>\n",
              "      <td>... gripping and handsome execution , -LRB- bu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  labels\n",
              "7842  Nothing more substantial than a fitfully cleve...       1\n",
              "2955  Last Orders nurtures the multi-layers of its c...       0\n",
              "7911                     The film seems a dead weight .       2\n",
              "3884  The performances are all solid ; it merely lac...       2\n",
              "2922  ... gripping and handsome execution , -LRB- bu...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaqbxPMo_1Wd"
      },
      "source": [
        "**Defining Fields**\n",
        "\n",
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequential to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hhr-dG5_yvp",
        "outputId": "06110adb-c2c6-45e6-86e8-3c019ffe251f"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7a914e68f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpmFFG7YAB9h"
      },
      "source": [
        "Tweet = data.Field(sequential=True, tokenize='spacy', batch_first=True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize='spacy', is_target=True, batch_first=True, sequential=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483p6kldAGur"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9by344WqAEbM"
      },
      "source": [
        "fields = [('tweets', Tweet),('labels',Label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbpe2a39AKx_"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhfQzW1MAJG_"
      },
      "source": [
        "# Creating dataset\n",
        "example = [data.Example.fromlist([sentence_sentiment.tweets[i],sentence_sentiment.labels[i]], fields) for i in range(sentence_sentiment.shape[0])] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLlAXklDrJn"
      },
      "source": [
        "## Save \n",
        "torch.save(example, 'example_v1.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaG9v3feCf-J"
      },
      "source": [
        "sentimentDataset = data.Dataset(example, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K5Nu8lyCqGc"
      },
      "source": [
        "##Splitting Datasets\n",
        "\n",
        "we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBZj-qS1Cnat"
      },
      "source": [
        "(train, valid, test) = sentimentDataset.split(split_ratio=[0.6, 0.2, 0.2], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgur4we-Cy63",
        "outputId": "06bb287b-cc41-46e5-f27e-bb1a9799df2a"
      },
      "source": [
        "len(train) ,len(test) ,len(valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7113, 2371, 2371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P0E-uJRC6ke",
        "outputId": "5529ceec-3800-4052-efdf-9a4807d518a1"
      },
      "source": [
        "vars(train.examples[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 1,\n",
              " 'tweets': ['A',\n",
              "  'confluence',\n",
              "  'of',\n",
              "  'kiddie',\n",
              "  'entertainment',\n",
              "  ',',\n",
              "  'sophisticated',\n",
              "  'wit',\n",
              "  'and',\n",
              "  'symbolic',\n",
              "  'graphic',\n",
              "  'design',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYY5XxeMDCIl"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qzii82FDB19",
        "outputId": "38b00034-348e-48ea-fc59-f3a01f0cdb28"
      },
      "source": [
        "# import libraries and prepare sentence text\n",
        "\n",
        "import random\n",
        "# for back translation\n",
        "!pip install google_trans_new\n",
        "import google_trans_new\n",
        "from google_trans_new import google_translator\n",
        "# for word tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_trans_new\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/7b/9f136106dc5824dc98185c97991d3cd9b53e70a197154dd49f7b899128f6/google_trans_new-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: google-trans-new\n",
            "Successfully installed google-trans-new-1.1.9\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uFEV7WvDO0c"
      },
      "source": [
        "Back Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLrSE8M9DAe8"
      },
      "source": [
        "#translate a sentence to a random language,\n",
        "# and translate back to original language\n",
        "\n",
        "def back_translate(sentence, p=0.1):\n",
        "  # do nothing with probability of 1-p\n",
        "  if random.uniform(0,1) > p:\n",
        "    return sentence\n",
        "\n",
        "  # combine tokenized sentence into one string\n",
        "  sentence = ' '.join(sentence)\n",
        "\n",
        "  # instantiate translator\n",
        "  translator = google_translator()\n",
        "\n",
        "  # choose a target language\n",
        "  available_langs = list(google_trans_new.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs) \n",
        "  #print(f\"Translating to {google_trans_new.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "  # translate to the target language\n",
        "  translations = translator.translate(sentence, lang_tgt=trans_lang) \n",
        "  #print(translations)\n",
        "\n",
        "  # translate back to original language\n",
        "  translations_en_random = translator.translate(translations, lang_src=trans_lang, lang_tgt='en') \n",
        "  #print(translations_en_random)\n",
        "\n",
        "  # select only one translation\n",
        "  if len(translations_en_random) > 1:\n",
        "    translations_en_random = translations_en_random[0]\n",
        "\n",
        "  return word_tokenize(translations_en_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E56-DzcgDWc8"
      },
      "source": [
        "Random Deletion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3boc5nnDUca"
      },
      "source": [
        "#randomly delete words from a sentence with a given probability\n",
        "\n",
        "def random_deletion(sentence, p=0.5): \n",
        "    # return if single word\n",
        "    if len(sentence) == 1: \n",
        "        return sentence\n",
        "    # delete words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p, sentence)) \n",
        "    # if nothing left, sample a random word\n",
        "    if len(remaining) == 0: \n",
        "        return [random.choice(sentence)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsRVd64GDblQ"
      },
      "source": [
        "Random Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmwZ1CNVDa3H"
      },
      "source": [
        "# randomly swap a pair of words in a sentence for a given # of times\n",
        "\n",
        "def random_swap(sentence, n=5): \n",
        "    if len(sentence) < 2:\n",
        "      return sentence\n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfYF628XDgRj"
      },
      "source": [
        "Data Augmentation Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPqvVQc4DfcC"
      },
      "source": [
        "for example in train.examples: \n",
        "  example.tweets = back_translate(example.tweets, p=0.01)\n",
        "  example.tweets = random_deletion(example.tweets, p=0.1)\n",
        "  example.tweets = random_swap(example.tweets, n=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2eh2_zNG_LV"
      },
      "source": [
        "Building Vocabulary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM9HEQspHDY8"
      },
      "source": [
        "## Building Vocabulary\n",
        "\n",
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabulary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all.\n",
        "Let’s limit the vocabulary to a maximum of 10,000 words in our training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8STPxSpNHH_t"
      },
      "source": [
        "MAX_VOCAB_SIZE = 10_000\n",
        "\n",
        "Tweet.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-JsJrhjHP2Y"
      },
      "source": [
        "By default, torchtext will add two more special tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwQ5-hB6HNQZ",
        "outputId": "1863776d-e41c-4a67-dc53-e6f376d6c687"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  10002\n",
            "Size of label vocab :  3\n",
            "Top 10 words appreared repeatedly : [('.', 5969), (',', 5273), ('the', 4519), ('and', 3323), ('of', 3244), ('a', 3208), ('to', 2260), ('-', 2036), (\"'s\", 1905), ('is', 1866)]\n",
            "Labels :  defaultdict(None, {0: 0, 1: 1, 2: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_BWHzDHHYKL"
      },
      "source": [
        "\n",
        "## Making batches\n",
        "\n",
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images.\n",
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeILz0WsHUz_"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qUHMAPvHUpQ"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size = 32, \n",
        "    sort_key = lambda x: len(x.tweets),\n",
        "    sort_within_batch=True, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLMnIagJHg43"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBLaa4kOHnP0"
      },
      "source": [
        "Build Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhCaowOOHmQe"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
        "                 n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                               hidden_dim, \n",
        "                               num_layers=n_layers, \n",
        "                               dropout=dropout,\n",
        "                               batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function (softmax)\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxPmY2hHvNj",
        "outputId": "3fa93c0a-6ae0-463b-b55f-9e186951b979"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 3\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(10002, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlflOLtFHwV7",
        "outputId": "f2c62c53-81ff-408f-8ae1-984b194ca655"
      },
      "source": [
        "# No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,242,503 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnpI9AV5H2vQ"
      },
      "source": [
        "Train Evaluate Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lcU64eyH2Tq"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j-CgYquH9Sb"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76aTFBrIHzNi"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OGlULJvIFKb"
      },
      "source": [
        "Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYbFZ_NxIDLm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UXRLBBtIKIB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZdBLfaKIJaE",
        "outputId": "b1feccae-6c8b-450b-ba4b-4459e21e6e34"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.011 | Train Acc: 54.26%\n",
            "\t Val. Loss: 0.989 |  Val. Acc: 56.25% \n",
            "\n",
            "\tTrain Loss: 0.977 | Train Acc: 56.63%\n",
            "\t Val. Loss: 0.976 |  Val. Acc: 56.38% \n",
            "\n",
            "\tTrain Loss: 0.900 | Train Acc: 63.01%\n",
            "\t Val. Loss: 0.842 |  Val. Acc: 70.46% \n",
            "\n",
            "\tTrain Loss: 0.806 | Train Acc: 74.38%\n",
            "\t Val. Loss: 0.806 |  Val. Acc: 74.33% \n",
            "\n",
            "\tTrain Loss: 0.779 | Train Acc: 76.91%\n",
            "\t Val. Loss: 0.821 |  Val. Acc: 72.29% \n",
            "\n",
            "\tTrain Loss: 0.762 | Train Acc: 78.30%\n",
            "\t Val. Loss: 0.800 |  Val. Acc: 74.21% \n",
            "\n",
            "\tTrain Loss: 0.751 | Train Acc: 79.52%\n",
            "\t Val. Loss: 0.797 |  Val. Acc: 73.67% \n",
            "\n",
            "\tTrain Loss: 0.737 | Train Acc: 81.86%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 78.00% \n",
            "\n",
            "\tTrain Loss: 0.723 | Train Acc: 83.42%\n",
            "\t Val. Loss: 0.763 |  Val. Acc: 78.96% \n",
            "\n",
            "\tTrain Loss: 0.713 | Train Acc: 84.28%\n",
            "\t Val. Loss: 0.751 |  Val. Acc: 80.04% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYC979j2IgFD",
        "outputId": "f516b2e5-3033-45eb-d119-3ccc0215d17e"
      },
      "source": [
        "# test model using testing dataset\n",
        "\n",
        "model.load_state_dict(torch.load('saved_weights.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.750 | Test Acc: 80.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMtOcYQAIjuW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAJK0N3hIjfS",
        "outputId": "22ca0123-217c-43f0-f453-ac8c6105f1b1"
      },
      "source": [
        "# load weights and tokenizer\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "# inference \n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "classifier(\n",
              "  (embedding): Embedding(10002, 300)\n",
              "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qMWypGN3Ioa-",
        "outputId": "fc9e87f5-b88c-485d-e7e8-475cc1fc18a6"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJdtlVWx7xDq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxANuLay7xAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}